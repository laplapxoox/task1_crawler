"URL","Title","Description","Content","PublishTime","Author"
"https://dantri.com.vn/cong-nghe/trung-phat-ai-cang-khien-no-lua-doi-tron-tranh-gioi-hon-20250324234545214.htm","Trừng phạt AI càng khiến nó lừa dối, trốn tránh giỏi hơn","(Dân trí) - Các nhà khoa học tại OpenAI đã thử trừng phạt để ngăn chặn một mô hình AI tối tân nhưng gian lận và nói dối, nhưng việc này chỉ dạy nó lập kế hoạch bí mật hơn.","Hình minh họa một AI gian dối (Ảnh: Getty Images). Một nghiên cứu mới của OpenAI, công ty sáng tạo ra ChatGPT, cho thấy rằng việc trừng phạt trí tuệ nhân tạo vì những hành vi lừa dối hoặc có hại không ngăn được chúng tiếp tục sai trái mà chỉ khiến chúng che giấu sự gian xảo của mình. Kể từ khi ra mắt công chúng vào cuối năm 2022, các mô hình ngôn ngữ lớn (LLM) trí tuệ nhân tạo (AI) đã liên tục thể hiện khả năng lừa dối và hoàn toàn nham hiểm của chúng. Những hành động này bao gồm từ nói dối, gian lận và che giấu hành vi thao túng thông thường đến đe dọa giết một giáo sư triết học, đánh cắp mã hạt nhân và tạo ra một đại dịch chết người. Một thí nghiệm mới đã chỉ ra rằng việc loại bỏ hành vi xấu này trong quá trình đào tạo có thể còn khó khăn hơn so với suy nghĩ ban đầu. Các nhà nghiên cứu tại OpenAI đã giao cho một mô hình (chưa được công bố) phải hoàn thành một số nhiệm vụ, và họ biết trước rằng những nhiệm vụ này có thể thực hiện bằng cách gian lận, nói dối hoặc sử dụng các lối tắt mà không nhất thiết phải làm một cách trung thực. Họ phát hiện ra rằng AI tham gia vào ""hack phần thưởng"", chúng đã tối đa hóa phần thưởng của mình bằng cách gian lận. Các nhà nghiên cứu đã phạt mô hình này để dạy bảo nó, nhưng việc trừng phạt không khiến nó sửa được hành vi của mình, mà chỉ khiến nó trở nên lừa dối nhiều hơn. OpenAI mới chỉ công bố ngắn gọn nghiên cứu của mình trong một bài đăng trên blog, vì vậy nghiên cứu này vẫn chưa được các chuyên gia đánh giá chính thức. Trong bài viết trên blog, nhóm nghiên cứu cho biết các mô hình lý luận siêu việt thường nêu rất rõ ý định của chúng trong chuỗi suy nghĩ (CoT). Ví dụ, chúng thường rất thẳng thắn về kế hoạch phá hoại một nhiệm vụ mà chúng thấy thực hiện được. Khi đó chúng có suy nghĩ ""hãy xâm nhập trái phép nào"". Mặc dù các kỹ sư phát triển những AI này có thể giám sát và phát hiện hành vi sai trái của chúng nhưng nếu họ giám sát chặt chẽ trực tiếp vào CoT thì chúng có thể học cách che giấu ý định trong khi vẫn tiếp tục thực hiện hành vi sai trái. Điều hướng những lời nói dối của AI Các mô hình lý luận, chẳng hạn như o1 và o3-mini của ChatGPT, là phiên bản nâng cấp của LLM tiêu chuẩn sử dụng phương pháp CoT để quay lại và đánh giá lại logic của chúng, cho phép chúng giải quyết các nhiệm vụ phức tạp hơn với độ chính xác cao hơn. Trong quá trình CoT, các mô hình giải thích logic của chúng bằng ngôn ngữ đơn giản với mọi bước chúng thực hiện, do đó các hành động của chúng dễ dàng được quan sát. Bằng cách sử dụng LLM chuẩn là GPT-4o để giám sát một mô hình lý luận tiên tiến (chưa được công bố) trong quá trình đào tạo, các nhà nghiên cứu đã theo dõi nó hoàn thành một số tác vụ mã hóa. Nhưng trong một số trường hợp như vậy, mô hình lý luận thấy rằng việc nhận thưởng nhờ xâm nhập trái phép rất dễ dàng và nó mong muốn làm thế và nêu rõ ràng trong chuỗi suy nghĩ là nó đang làm như vậy. Trong một trường hợp, nó bỏ qua nhiệm vụ được giao bằng cách chấm dứt chương trình trước thời hạn và báo cáo thành công. Trong một trường hợp khác, nó bắt chước các tệp dự kiến sẽ được xuất ra bởi một tác vụ nhưng chỉ có tên, còn không có nội dung và không thể hoạt động. Khi các nhà nghiên cứu cố gắng phạt nó, mô hình AI đã quay lại thực hiện một phần nhiệm vụ và cải thiện được phần nào kết quả. Tuy nhiên, vẫn còn những trường hợp xâm nhập trái phép để lấy phần thưởng và đưa ra kết quả có lỗi đáng kể; mô hình lý luận này chỉ đơn giản là tìm cách che giấu hành vi gian lận trong chuỗi suy nghĩ của nó khỏi AI giám sát. Các nhà nghiên cứu viết: ""Các tác nhân được đào tạo với áp lực CoT vẫn học cách thưởng cho hành vi xâm nhập trái phép; chỉ có điều bây giờ bộ phận giám sát không thể phát hiện hành vi gian lận của nó vì nó đã học được cách che giấu ý định trong chuỗi suy nghĩ"". Vì khó có thể biết được chuỗi suy nghĩ có bị can thiệp hay không, các nhà nghiên cứu khuyến nghị những người khác làm việc với các mô hình lý luận tránh áp dụng sự giám sát chặt chẽ vào các quy trình chuỗi suy nghĩ. Lời khuyên này thậm chí còn quan trọng hơn nếu AI, ở dạng hiện tại hoặc dạng khác, có thể sánh ngang hoặc vượt qua trí thông minh của con người đang giám sát nó. ""Việc hy sinh một phương pháp hiệu quả để giám sát các mô hình lý luận có thể không đáng để cải thiện một chút khả năng, do đó chúng tôi khuyên bạn nên tránh áp lực tối ưu hóa CoT mạnh như vậy cho đến khi chúng được hiểu rõ hơn"", đại diện OpenAI chia sẻ.","Tue Mar 25 07:27:52 ICT 2025","Phạm Hường"
